{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course Work 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-6pSVyWCSgX",
        "outputId": "714543f0-699d-402c-d2d0-2ca73db17442"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
        "def cache(cache_path, fn, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Cache-wrapper for a function or class. If the cache-file exists\n",
        "    then the data is reloaded and returned, otherwise the function\n",
        "    is called and the result is saved to cache. The fn-argument can\n",
        "    also be a class instead, in which case an object-instance is\n",
        "    created and saved to the cache-file.\n",
        "    :param cache_path:\n",
        "        File-path for the cache-file.\n",
        "    :param fn:\n",
        "        Function or class to be called.\n",
        "    :param args:\n",
        "        Arguments to the function or class-init.\n",
        "    :param kwargs:\n",
        "        Keyword arguments to the function or class-init.\n",
        "    :return:\n",
        "        The result of calling the function or creating the object-instance.\n",
        "    \"\"\"\n",
        "\n",
        "    # If the cache-file exists.\n",
        "    if os.path.exists(cache_path):\n",
        "        # Load the cached data from the file.\n",
        "        with open(cache_path, mode='rb') as file:\n",
        "            obj = pickle.load(file)\n",
        "\n",
        "        print(\"- Data loaded from cache-file: \" + cache_path)\n",
        "    else:\n",
        "        # The cache-file does not exist.\n",
        "\n",
        "        # Call the function / class-init with the supplied arguments.\n",
        "        obj = fn(*args, **kwargs)\n",
        "\n",
        "        # Save the data to a cache-file.\n",
        "        with open(cache_path, mode='wb') as file:\n",
        "            pickle.dump(obj, file)\n",
        "\n",
        "        print(\"- Data saved to cache-file: \" + cache_path)\n",
        "\n",
        "    return obj\n",
        "def convert_numpy2pickle(in_path, out_path):\n",
        "    \"\"\"\n",
        "    Convert a numpy-file to pickle-file.\n",
        "    The first version of the cache-function used numpy for saving the data.\n",
        "    Instead of re-calculating all the data, you can just convert the\n",
        "    cache-file using this function.\n",
        "    :param in_path:\n",
        "        Input file in numpy-format written using numpy.save().\n",
        "    :param out_path:\n",
        "        Output file written as a pickle-file.\n",
        "    :return:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    # Load the data using numpy.\n",
        "    data = np.load(in_path)\n",
        "\n",
        "    # Save the data using pickle.\n",
        "    with open(out_path, mode='wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is a short example of using a cache-file.\n",
        "\n",
        "    # This is the function that will only get called if the result\n",
        "    # is not already saved in the cache-file. This would normally\n",
        "    # be a function that takes a long time to compute, or if you\n",
        "    # need persistent data for some other reason.\n",
        "    def expensive_function(a, b):\n",
        "        return a * b\n",
        "\n",
        "    print('Computing expensive_function() ...')\n",
        "\n",
        "    # Either load the result from a cache-file if it already exists,\n",
        "    # otherwise calculate expensive_function(a=123, b=456) and\n",
        "    # save the result to the cache-file for next time.\n",
        "    result = cache(cache_path='cache_expensive_function.pkl',\n",
        "                   fn=expensive_function, a=123, b=456)\n",
        "\n",
        "    print('result =', result)\n",
        "\n",
        "    # Newline.\n",
        "    print()\n",
        "\n",
        "    # This is another example which saves an object to a cache-file.\n",
        "\n",
        "    # We want to cache an object-instance of this class.\n",
        "    # The motivation is to do an expensive computation only once,\n",
        "    # or if we need to persist the data for some other reason.\n",
        "    class ExpensiveClass:\n",
        "        def __init__(self, c, d):\n",
        "            self.c = c\n",
        "            self.d = d\n",
        "            self.result = c * d\n",
        "\n",
        "        def print_result(self):\n",
        "            print('c =', self.c)\n",
        "            print('d =', self.d)\n",
        "            print('result = c * d =', self.result)\n",
        "\n",
        "    print('Creating object from ExpensiveClass() ...')\n",
        "\n",
        "    # Either load the object from a cache-file if it already exists,\n",
        "    # otherwise make an object-instance ExpensiveClass(c=123, d=456)\n",
        "    # and save the object to the cache-file for the next time.\n",
        "    obj = cache(cache_path='cache_ExpensiveClass.pkl',\n",
        "                fn=ExpensiveClass, c=123, d=456)\n",
        "\n",
        "    obj.print_result()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing expensive_function() ...\n",
            "- Data loaded from cache-file: cache_expensive_function.pkl\n",
            "result = 56088\n",
            "\n",
            "Creating object from ExpensiveClass() ...\n",
            "- Data loaded from cache-file: cache_ExpensiveClass.pkl\n",
            "c = 123\n",
            "d = 456\n",
            "result = c * d = 56088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iY4SDtC5rE1M",
        "outputId": "38b0863a-7326-4c92-da0f-f4810fb95732"
      },
      "source": [
        "!pip install sobol_seq\n",
        "!pip install tensorflow==1.15.5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sobol_seq\n",
            "  Downloading https://files.pythonhosted.org/packages/e4/df/6c4ad25c0b48545a537b631030f7de7e4abb939e6d2964ac2169d4379c85/sobol_seq-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sobol_seq) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sobol_seq) (1.19.5)\n",
            "Installing collected packages: sobol-seq\n",
            "Successfully installed sobol-seq-0.2.0\n",
            "Collecting tensorflow==1.15.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/51/99abd43185d94adaaaddf8f44a80c418a91977924a7bc39b8dacd0c495b0/tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 94kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.32.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.2)\n",
            "Requirement already satisfied: h5py<=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.12.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 35.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.36.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (1.15.0)\n",
            "Collecting numpy<1.19.0,>=1.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1MB 1.5MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 37.6MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.5) (56.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5) (3.4.1)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=3a43c77834ec7002fc4c21499d8b48b5cd3ffe4aa0b1e29cab2cdf4595445f97\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, numpy, keras-applications, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "numpy",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxn9Wy3uChKy"
      },
      "source": [
        "import tensorflow as tf\n",
        "from scipy.stats import norm\n",
        "import sobol_seq\n",
        "\n",
        "DTYPE = tf.float32\n",
        "\n",
        "class Compressible(object):\n",
        "    def __init__(self, name, message_freq=1000):\n",
        "        self.name = name\n",
        "        self.message_freq = message_freq\n",
        "        self.message_counter = 0\n",
        "\n",
        "    def get_feed_dict(self, validation=False):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def get_train_op(self, training):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def training_step(self, training, extra_ops):\n",
        "        self.message_counter += 1\n",
        "        if self.message_counter % self.message_freq == 0 or self.message_counter == 1:\n",
        "            loss, training_acc, kl, kl_loss = self.sess.run([self.loss, self.accuracy, self.mean_kl, self.kl_loss],\n",
        "                                                   feed_dict=self.get_feed_dict())\n",
        "\n",
        "            mean_validation_acc = 0.0\n",
        "            for i in range(10):\n",
        "                validation_acc = self.sess.run(self.accuracy,\n",
        "                                               feed_dict=self.get_feed_dict(validation=True))\n",
        "                mean_validation_acc += validation_acc\n",
        "            mean_validation_acc /= 10.\n",
        "            print(\"Iteration {}, Validation score = {}, Training score = {}, Loss = {}, KL-Loss = {}, KL_2 = {}\".format(\n",
        "                self.message_counter,\n",
        "                mean_validation_acc,\n",
        "                training_acc,\n",
        "                loss,\n",
        "                kl_loss,\n",
        "                kl / np.log(2.)))\n",
        "            path = '/scratch/mh740/compression_models/{}/{}/{}.ckpt'.format(self.name, training,\n",
        "                                                                         self.message_counter)\n",
        "            if not os.path.exists(path):\n",
        "                os.makedirs(path)\n",
        "            self.saver.save(self.sess, path)\n",
        "\n",
        "        self.sess.run((self.get_train_op(training=training), extra_ops),\n",
        "                      feed_dict=self.get_feed_dict())\n",
        "\n",
        "    def train(self, iterations, enforce_kl):\n",
        "        if enforce_kl:\n",
        "            self.sess.run(self.enable_kl_loss.assign(1.))\n",
        "            with tf.control_dependencies([self.get_train_op(training='training')]):\n",
        "                extra_ops = [tf.identity(self.kl_penalty_update)]\n",
        "            for i in range(iterations):\n",
        "                self.training_step(training='training', extra_ops=extra_ops)\n",
        "        else:\n",
        "            self.sess.run(self.enable_kl_loss.assign(0.))\n",
        "            for i in range(iterations):\n",
        "                self.training_step(training='pretrain', extra_ops=[])\n",
        "        mean_validation_acc = 0.0\n",
        "        for i in range(20):\n",
        "            validation_acc = self.sess.run(self.accuracy,\n",
        "                                           feed_dict=self.get_feed_dict(validation=True))\n",
        "        mean_validation_acc += validation_acc\n",
        "        return mean_validation_acc / 20.\n",
        "\n",
        "    def compress(self, retrain_iter, kl_penalty_step=1.0005):\n",
        "        self.sess.run(self.kl_penalty_step.assign(kl_penalty_step))\n",
        "        n_blocks = self.fixed_weights.get_shape().as_list()[0]\n",
        "        self.sess.run(self.enable_kl_loss.assign(1.))\n",
        "        for i in range(n_blocks):\n",
        "            self.sess.run(self.comp_ops, feed_dict={self.block_to_comp: i})\n",
        "            print('Block {} of {} compressed'.format(i, n_blocks))\n",
        "            for j in range(retrain_iter):\n",
        "                self.training_step(training='compression', extra_ops=self.kl_penalty_update)\n",
        "\n",
        "        mean_validation_acc = 0.0\n",
        "        for i in range(100):\n",
        "            validation_acc = self.sess.run(self.accuracy,\n",
        "                                           feed_dict=self.get_feed_dict(validation=True))\n",
        "            mean_validation_acc += validation_acc\n",
        "        return mean_validation_acc / 100.\n",
        "\n",
        "    def initialize_variables(self,\n",
        "                             dimensions,\n",
        "                             initializers,\n",
        "                             hash_group_sizes,\n",
        "                             block_size,\n",
        "                             bits_per_block,\n",
        "                             weight_decay=5e-4,\n",
        "                             kl_penalty_step=1.00005):\n",
        "        assert len(initializers) == len(dimensions)\n",
        "        num_vars = 0\n",
        "        for dim, group_size in zip(dimensions, hash_group_sizes):\n",
        "            assert np.prod(dim) % group_size == 0\n",
        "            num_vars += np.prod(dim) / group_size\n",
        "        n_blocks = np.int64(1 + (num_vars - 1) / block_size)\n",
        "        shape = [n_blocks, block_size]\n",
        "        print('Number of blocks: {}, Block size: {}, Bits per block: {}, Target KL: {}, Overall bits {}, Ratio: {}'.format(\n",
        "            n_blocks, block_size, bits_per_block, bits_per_block, bits_per_block*n_blocks, np.sum([np.prod(dim) for dim in dimensions])*32. / (bits_per_block * n_blocks)\n",
        "        ))\n",
        "        num_vars_ub = np.prod(shape)\n",
        "\n",
        "        np.random.seed(420)\n",
        "        num_vars_ub = np.int64(num_vars_ub)\n",
        "        num_vars = np.int64(num_vars)\n",
        "        permutation = np.random.permutation(num_vars_ub)\n",
        "        permutation_inv = np.argsort(permutation)\n",
        "        var_sizes = [np.prod(dim)/group_size for dim, group_size in zip(dimensions, hash_group_sizes)]\n",
        "        var_sizes = np.int64(var_sizes)\n",
        "        # print(var_sizes)\n",
        "        # print(initializers)\n",
        "\n",
        "        self.p_scale_vars = tf.Variable(tf.fill([len(dimensions) + 1], -2.), dtype=DTYPE)\n",
        "        print(range(len(dimensions) + 1))\n",
        "        #p_perm_inv = np.repeat(range(len(dimensions) + 1), var_sizes + [num_vars_ub - num_vars])[permutation_inv]\n",
        "        p_perm_inv = np.repeat(range(len(dimensions)), var_sizes + [num_vars_ub - num_vars])[permutation_inv]\n",
        "        #self.p_scale = tf.reshape(tf.gather(tf.exp(self.p_scale_vars), p_perm_inv), (shape))\n",
        "        shape = np.int64(shape)\n",
        "        self.p_scale = tf.reshape(tf.gather(tf.exp(self.p_scale_vars), p_perm_inv), (shape))\n",
        "        p = tf.contrib.distributions.Normal(loc=0., scale=self.p_scale)\n",
        "        mu_init_list = []\n",
        "        for (type, val), size in zip(initializers, var_sizes):\n",
        "            if type == 'normal':\n",
        "                mu_init_list.append(np.random.normal(size=size, loc=0., scale=val))\n",
        "            elif type == 'uni':\n",
        "                mu_init_list.append(np.random.uniform(-val, val, size=size))\n",
        "            elif type == 'zero':\n",
        "                mu_init_list.append(np.zeros(size))\n",
        "            else:\n",
        "                assert False\n",
        "\n",
        "        mu_init = np.concatenate(mu_init_list)\n",
        "        # print(num_vars, mu_init.shape)\n",
        "        # print(var_sizes, [init.shape for init in mu_init_list])\n",
        "        init_inv_permuted = np.concatenate((mu_init,\n",
        "                                            np.zeros(num_vars_ub - num_vars)),\n",
        "                                           axis=0)[permutation_inv]\n",
        "\n",
        "        mu = tf.Variable(init_inv_permuted.reshape(shape), dtype=DTYPE, name='mu')\n",
        "        self.mu = mu\n",
        "        self.weight_decay_loss = tf.reduce_sum(tf.square(mu)) * weight_decay\n",
        "        self.sigma_var = tf.Variable(tf.fill(shape, tf.cast(-10., dtype=DTYPE, name='sigma')))\n",
        "        sigma = tf.exp(self.sigma_var)\n",
        "        self.sigma = sigma\n",
        "        epsilon = tf.random_normal(shape)\n",
        "        self.w_dist = tf.contrib.distributions.Normal(loc=mu, scale=sigma)\n",
        "        variational_weights = mu + epsilon * sigma\n",
        "        self.fixed_weights = tf.Variable(tf.zeros_like(variational_weights), trainable=False)\n",
        "        self.mask = tf.Variable(tf.ones([n_blocks]), trainable=False)\n",
        "        kl_penalties = tf.Variable(tf.fill([n_blocks], tf.cast(1e-8, dtype=DTYPE)), trainable=False)\n",
        "        self.kl_penalties = kl_penalties\n",
        "\n",
        "        kl_target = tf.Variable(bits_per_block * np.log(2.), dtype=tf.float32, trainable=False)\n",
        "        block_kl = tf.reduce_sum(tf.distributions.kl_divergence(self.w_dist, p), axis=1)\n",
        "        self.mean_kl = tf.reduce_mean(block_kl)\n",
        "\n",
        "        self.enable_kl_loss = tf.Variable(1., dtype=DTYPE, trainable=False)\n",
        "        self.kl_loss = tf.reduce_sum(block_kl * self.mask * kl_penalties) * self.enable_kl_loss\n",
        "        self.kl_penalty_step = tf.Variable(kl_penalty_step, trainable=False)\n",
        "        self.kl_penalty_update = [kl_penalties.assign(tf.where(tf.logical_and(tf.cast(self.mask, tf.bool),\n",
        "                                                                              tf.greater(block_kl, kl_target)),\n",
        "                                                               kl_penalties * self.kl_penalty_step,\n",
        "                                                               kl_penalties / self.kl_penalty_step))]\n",
        "\n",
        "        mask_expanded = tf.expand_dims(self.mask, 1)\n",
        "        combined_weights = tf.reshape(mask_expanded * variational_weights\n",
        "                                      + (1. - mask_expanded) * self.fixed_weights,\n",
        "                                      [-1])\n",
        "\n",
        "        permuted_weights = tf.gather(combined_weights, permutation)\n",
        "        split_weights = tf.split(permuted_weights, var_sizes + [num_vars_ub - num_vars])\n",
        "\n",
        "        result = []\n",
        "        i = 0\n",
        "        for dim in dimensions:\n",
        "            split = tf.expand_dims(split_weights[i], axis=1) * np.random.choice([-1., 1.], size=hash_group_sizes[i])\n",
        "            # print(split.get_shape().as_list())\n",
        "            result.append(tf.reshape(split, dim))\n",
        "            i += 1\n",
        "\n",
        "        self.initialize_compressor(bits_per_block)\n",
        "        return result\n",
        "\n",
        "    def initialize_compressor(self, bits_per_block):\n",
        "        with tf.variable_scope('compressor'):\n",
        "            self.block_to_comp = tf.placeholder(tf.int32)\n",
        "            shape = self.fixed_weights.get_shape().as_list()\n",
        "            # block_ind = tf.expand_dims(block, 1)\n",
        "            # mask = tf.scatter_nd(block_ind, tf.ones([1], dtype=tf.float32), shape)\n",
        "            # sample_shape = tf.concat(([tries], shape), axis=0)\n",
        "\n",
        "            # sequencer = ghalton.Halton(shape[1])\n",
        "            n_blocks = shape[0]\n",
        "            sobol_dim = shape[1]\n",
        "            assert sobol_dim <= 40\n",
        "            uni_quasi = np.array(sobol_seq.i4_sobol_generate(sobol_dim, np.power(2, bits_per_block), skip=1)).transpose()\n",
        "            normal_quasi = norm.ppf(uni_quasi).transpose()\n",
        "            #normal_quasi = np.tile(normal_quasi[:, None, :], [1, n_blocks, 1])\n",
        "            # This line helps but not exactly sure why\n",
        "            # normal_quasi /= np.sqrt(np.mean(np.square(normal_quasi), axis=1))[:, None]\n",
        "            sample_block = tf.constant(normal_quasi, dtype=DTYPE)\n",
        "\n",
        "            # normal = np.random.normal(size=(tries, shape[0], shape[1]))\n",
        "            # normal /= np.sqrt(np.mean(np.square(normal), axis=2))[:, :, None]\n",
        "            # sample_block = tf.constant(normal, dtype=tf.float32) * p_scale\n",
        "\n",
        "            block_p = self.p_scale[self.block_to_comp, :]\n",
        "            block_mu = self.mu[self.block_to_comp, :]\n",
        "            block_sigma = self.sigma[self.block_to_comp, :]\n",
        "            nll_q = tf.reduce_sum(tf.square(block_mu - sample_block * block_p) / (2*tf.square(block_sigma)), axis=1)\n",
        "            nll_p = tf.reduce_sum(tf.square(sample_block), axis=1)\n",
        "            #prob = tf.Print(tf.exp(-nll), [nll_q, nll_p], summarize=100)\n",
        "            #norm_prob = tf.Print(prob / tf.reduce_sum(prob), [prob], summarize=100)\n",
        "            dist = tf.distributions.Categorical(probs=tf.nn.softmax(nll_p - nll_q)) # , validate_args=True) #Risky\n",
        "            index = dist.sample([])\n",
        "\n",
        "            # This line makes the algorithm objectively better. But we cannot prove it theoretically.\n",
        "            # min_index = tf.argmin(nll_q, axis=0)\n",
        "\n",
        "            best_sample = sample_block[index, :] * block_p\n",
        "            self.comp_ops = []\n",
        "            self.comp_ops.append(tf.scatter_update(self.fixed_weights,\n",
        "                                                   [self.block_to_comp],\n",
        "                                                   [best_sample]))\n",
        "            self.comp_ops.append(tf.scatter_update(self.mask, [self.block_to_comp], [0.]))\n",
        "\n",
        "    def initialize_session(self, load_name=None):\n",
        "        # Initialize the variables (i.e. assign their default value)\n",
        "        self.saver = tf.train.Saver(max_to_keep=None)\n",
        "        self.loader = tf.train.Saver(var_list=[v for v in tf.all_variables() if v not in []])\n",
        "        init = tf.global_variables_initializer()\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "\n",
        "        self.sess = tf.Session(config=config)\n",
        "\n",
        "        # Run the initializer\n",
        "        self.sess.run(init)\n",
        "\n",
        "        if load_name is not None:\n",
        "            # tf.reset_default_graph()\n",
        "            path = '/scratch/mh740/compression_models/{}'.format(load_name)\n",
        "            self.loader.restore(self.sess, path)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qHBMueUDD_z"
      },
      "source": [
        "########################################################################\n",
        "#\n",
        "# Functions for downloading and extracting data-files from the internet.\n",
        "#\n",
        "# Implemented in Python 3.5\n",
        "#\n",
        "########################################################################\n",
        "#\n",
        "# This code is part of the TensorFlow Tutorials available at:\n",
        "#\n",
        "# https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
        "#\n",
        "# Published under the MIT License. See the file LICENSE for details.\n",
        "#\n",
        "# Copyright 2016 by Magnus Erik Hvass Pedersen\n",
        "#\n",
        "########################################################################\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import urllib\n",
        "import tarfile\n",
        "import zipfile\n",
        "\n",
        "########################################################################\n",
        "\n",
        "\n",
        "def _print_download_progress(count, block_size, total_size):\n",
        "    \"\"\"\n",
        "    Function used for printing the download progress.\n",
        "    Used as a call-back function in maybe_download_and_extract().\n",
        "    \"\"\"\n",
        "\n",
        "    # Percentage completion.\n",
        "    pct_complete = float(count * block_size) / total_size\n",
        "\n",
        "    # Limit it because rounding errors may cause it to exceed 100%.\n",
        "    pct_complete = min(1.0, pct_complete)\n",
        "\n",
        "    # Status-message. Note the \\r which means the line should overwrite itself.\n",
        "    msg = \"\\r- Download progress: {0:.1%}\".format(pct_complete)\n",
        "\n",
        "    # Print it.\n",
        "    sys.stdout.write(msg)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "########################################################################\n",
        "\n",
        "def download(base_url, filename, download_dir):\n",
        "    \"\"\"\n",
        "    Download the given file if it does not already exist in the download_dir.\n",
        "    :param base_url: The internet URL without the filename.\n",
        "    :param filename: The filename that will be added to the base_url.\n",
        "    :param download_dir: Local directory for storing the file.\n",
        "    :return: Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Path for local file.\n",
        "    save_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists, otherwise we need to download it now.\n",
        "    if not os.path.exists(save_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        print(\"Downloading\", filename, \"...\")\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        url = base_url + filename\n",
        "        file_path, _ = urllib.urlretrieve(url=url,\n",
        "                                          filename=save_path,\n",
        "                                          reporthook=_print_download_progress)\n",
        "\n",
        "        print(\" Done!\")\n",
        "\n",
        "\n",
        "def maybe_download_and_extract(url, download_dir):\n",
        "    \"\"\"\n",
        "    Download and extract the data if it doesn't already exist.\n",
        "    Assumes the url is a tar-ball file.\n",
        "    :param url:\n",
        "        Internet URL for the tar-file to download.\n",
        "        Example: \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    :param download_dir:\n",
        "        Directory where the downloaded file is saved.\n",
        "        Example: \"data/CIFAR-10/\"\n",
        "    :return:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Filename for saving the file downloaded from the internet.\n",
        "    # Use the filename from the URL and add it to the download_dir.\n",
        "    filename = url.split('/')[-1]\n",
        "    file_path = os.path.join(download_dir, filename)\n",
        "\n",
        "    # Check if the file already exists.\n",
        "    # If it exists then we assume it has also been extracted,\n",
        "    # otherwise we need to download and extract it now.\n",
        "    if not os.path.exists(file_path):\n",
        "        # Check if the download directory exists, otherwise create it.\n",
        "        if not os.path.exists(download_dir):\n",
        "            os.makedirs(download_dir)\n",
        "\n",
        "        # Download the file from the internet.\n",
        "        file_path, _ = urllib.urlretrieve(url=url,\n",
        "                                          filename=file_path,\n",
        "                                          reporthook=_print_download_progress)\n",
        "\n",
        "        print()\n",
        "        print(\"Download finished. Extracting files.\")\n",
        "\n",
        "        if file_path.endswith(\".zip\"):\n",
        "            # Unpack the zip-file.\n",
        "            zipfile.ZipFile(file=file_path, mode=\"r\").extractall(download_dir)\n",
        "        elif file_path.endswith((\".tar.gz\", \".tgz\")):\n",
        "            # Unpack the tar-ball.\n",
        "            tarfile.open(name=file_path, mode=\"r:gz\").extractall(download_dir)\n",
        "\n",
        "        print(\"Done.\")\n",
        "    else:\n",
        "        print(\"Data has apparently already been downloaded and unpacked.\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5LMUyBJDbs2",
        "outputId": "e07dc1e0-6ad6-435b-eb4f-11778d7d566b"
      },
      "source": [
        "#!pip install tensorflow==1.15.5\n",
        "import tensorflow as tf\n",
        "print(tf.__version__) #must be 1.15.5!"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p68uQ4pDRpu"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "class Lenet5(Compressible):\n",
        "    def conv2d(self, x, W, b, padding='SAME', strides=1):\n",
        "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=padding)\n",
        "        x = tf.nn.bias_add(x, b)\n",
        "        return tf.nn.relu(x)\n",
        "\n",
        "    def maxpool2d(self, x, k=2, padding='SAME'):\n",
        "        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                              padding=padding)\n",
        "\n",
        "    # Создаем модель\n",
        "    def conv_net(self, x, weights):\n",
        "        # MNIST подается на вход как вектор (1, 784)\n",
        "        # Делаем RESHAPE для соответствия формату изображения [Height, Width, Channel]\n",
        "        # Входной тензор становится следующим: [Batch Size, Height, Width, Channel]\n",
        "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "        # Сверточный слой\n",
        "        conv1 = self.conv2d(x, weights['wc1'], weights['bc1'], padding='VALID')\n",
        "        print(conv1.shape)\n",
        "        # Maxpool\n",
        "        conv1 = self.maxpool2d(conv1, k=2, padding='SAME')\n",
        "        print(conv1.shape)\n",
        "        # Сверточный слой\n",
        "        conv2 = self.conv2d(conv1, weights['wc2'], weights['bc2'], padding='VALID')\n",
        "        print(conv2.shape)\n",
        "        # Maxpool\n",
        "        conv2 = self.maxpool2d(conv2, k=2, padding='SAME')\n",
        "        print(conv2.shape)\n",
        "\n",
        "        # Полносвязный слой\n",
        "        # Изменение выхода conv2, чтобы соответствовать входу полносвязаного\n",
        "        fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "        fc1 = tf.add(tf.matmul(fc1, weights['wd1']), weights['bd1'])\n",
        "        fc1 = tf.nn.relu(fc1)\n",
        "\n",
        "        # Выход, предсказание класса\n",
        "        out = tf.add(tf.matmul(fc1, weights['out']), weights['bout'])\n",
        "        return out\n",
        "\n",
        "    def __init__(self, bpb, load_name=None):\n",
        "        super(Lenet5, self).__init__('Lenet5')\n",
        "        self.mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
        "\n",
        "        # Обучающие параметры\n",
        "        self.batch_size = 256\n",
        "\n",
        "        # Параметры сети\n",
        "        num_input = 784  # MNIST data input (img shape: 28*28)\n",
        "        num_classes = 10  # MNIST total classes (0-9 digits)\n",
        "\n",
        "        # tf Ввод графика\n",
        "        self.X = tf.placeholder(tf.float32, [None, num_input]) - 0.5\n",
        "        self.Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "\n",
        "        # Веса\n",
        "        weight_names = ['wc1', 'wc2', 'wd1', 'out', 'bc1', 'bc2', 'bd1', 'bout']\n",
        "        weight_dims = [[5, 5, 1, 20], [5, 5, 20, 50], [4 * 4 * 50, 500],\n",
        "                       [500, num_classes], [20], [50], [500], [num_classes]]\n",
        "        weight_hash_groups = [1, 2, 50, 1, 1, 1, 1, 1]\n",
        "        weight_initializers = []\n",
        "        for d in weight_dims:\n",
        "            if len(d) == 4:\n",
        "                weight_initializers.append(('normal', np.sqrt(1. / (d[0] * d[1] * d[2]))))\n",
        "            else:\n",
        "                weight_initializers.append(('normal', np.sqrt(1. / d[0])))\n",
        "\n",
        "        weights = {}\n",
        "        weights.update(zip(weight_names, self.initialize_variables(weight_dims,\n",
        "                                                                   weight_initializers,\n",
        "                                                                   weight_hash_groups,\n",
        "                                                                   30, bpb,\n",
        "                                                                   kl_penalty_step=1.0001)))\n",
        "        # Строим модель\n",
        "        logits = self.conv_net(self.X, weights)\n",
        "\n",
        "        # Оцениваем модель\n",
        "        prediction = tf.nn.softmax(logits)\n",
        "        correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_pred, DTYPE))\n",
        "\n",
        "        # Определяем loss и оптимизатор\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=self.Y)) + self.kl_loss\n",
        "\n",
        "        global_step = tf.Variable(initial_value=0,\n",
        "                                  name='global_step', trainable=False)\n",
        "        learning_rate = tf.train.exponential_decay(\n",
        "            0.001,  # Базовый learning rate.\n",
        "            global_step,  # Текущий индекс в датасете\n",
        "            30 * self.mnist.train.images.shape[0] / self.batch_size,  # Шаг\n",
        "            1.,  # Скорость\n",
        "            staircase=True)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        self.train_op = optimizer.minimize(self.loss)\n",
        "        no_scales_list = [v for v in tf.trainable_variables() if v is not self.p_scale_vars]\n",
        "        assert len(no_scales_list) < len(tf.trainable_variables())\n",
        "        self.train_op_no_scales = optimizer.minimize(self.loss, var_list=no_scales_list)\n",
        "\n",
        "        self.initialize_session(load_name)\n",
        "\n",
        "    def get_feed_dict(self, validation=False):\n",
        "        if validation:\n",
        "            batch_x, batch_y = self.mnist.validation.images, self.mnist.validation.labels\n",
        "        else:\n",
        "            batch_x, batch_y = self.mnist.train.next_batch(self.batch_size)\n",
        "        return {self.X: batch_x, self.Y: batch_y}\n",
        "\n",
        "    def get_train_op(self, training=True):\n",
        "        if training:\n",
        "            return self.train_op\n",
        "        else:\n",
        "            return self.train_op_no_scales"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFhf_bh7DWDc",
        "outputId": "debf6b99-220e-488a-c482-41dbf1d51e69"
      },
      "source": [
        "# Обучаем LeNet-5 на MNIST\n",
        "model = Lenet5(bpb=10)\n",
        "\n",
        "model.train(200000, False)\n",
        "model.train(200000, True)\n",
        "# переобучаем,\n",
        "print(model.compress(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-34ab40d23257>:46: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Number of blocks: 886, Block size: 30, Bits per block: 10, Target KL: 10, Overall bits 8860, Ratio: 1556.9480812641084\n",
            "range(0, 9)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-247ab2e87488>:121: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/distributions/normal.py:160: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From <ipython-input-2-247ab2e87488>:155: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From <ipython-input-2-247ab2e87488>:164: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From <ipython-input-2-247ab2e87488>:215: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "(?, 24, 24, 20)\n",
            "(?, 12, 12, 20)\n",
            "(?, 8, 8, 50)\n",
            "(?, 4, 4, 50)\n",
            "WARNING:tensorflow:From <ipython-input-6-34ab40d23257>:87: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-247ab2e87488>:231: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Please use tf.global_variables instead.\n",
            "Iteration 1, Validation = 0.0989999994635582, Training = 0.1015625, Loss = 2.4404590129852295, KL-Loss = 0.0, KL_2 = 327.7253624940578\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/1.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 1000, Validation = 0.9875200152397156, Training = 0.9921875, Loss = 0.030473969876766205, KL-Loss = 0.0, KL_2 = 329.4779895338584\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/1000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 2000, Validation = 0.9905999898910522, Training = 0.98828125, Loss = 0.030603472143411636, KL-Loss = 0.0, KL_2 = 330.5291254821526\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/2000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 3000, Validation = 0.9908000230789185, Training = 0.99609375, Loss = 0.01166346576064825, KL-Loss = 0.0, KL_2 = 331.5272962774259\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/3000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 4000, Validation = 0.9924399971961975, Training = 1.0, Loss = 0.003296226728707552, KL-Loss = 0.0, KL_2 = 332.48696497268514\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/4000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 5000, Validation = 0.989659994840622, Training = 0.99609375, Loss = 0.007495228201150894, KL-Loss = 0.0, KL_2 = 333.4730722168963\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/5000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 6000, Validation = 0.9926000237464905, Training = 1.0, Loss = 0.0009532078984193504, KL-Loss = 0.0, KL_2 = 334.35591282236214\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/6000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 7000, Validation = 0.9914000034332275, Training = 1.0, Loss = 0.0018323732074350119, KL-Loss = 0.0, KL_2 = 335.4833925573049\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/7000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 8000, Validation = 0.9898799896240235, Training = 1.0, Loss = 0.000754200853407383, KL-Loss = 0.0, KL_2 = 336.4605842208953\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/8000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 9000, Validation = 0.9909999966621399, Training = 0.99609375, Loss = 0.011843733489513397, KL-Loss = 0.0, KL_2 = 337.45864494727203\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/9000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 10000, Validation = 0.9907200038433075, Training = 1.0, Loss = 0.00014291770639829338, KL-Loss = 0.0, KL_2 = 338.32473306668265\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/10000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 11000, Validation = 0.9916399896144867, Training = 1.0, Loss = 6.662374653387815e-05, KL-Loss = 0.0, KL_2 = 339.5010173503567\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/11000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 12000, Validation = 0.9927600026130676, Training = 1.0, Loss = 0.0015270349103957415, KL-Loss = 0.0, KL_2 = 340.3584100269397\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/12000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 13000, Validation = 0.9909000098705292, Training = 1.0, Loss = 6.299512460827827e-05, KL-Loss = 0.0, KL_2 = 341.3007538778817\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/13000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 14000, Validation = 0.991599977016449, Training = 1.0, Loss = 0.0007704099989496171, KL-Loss = 0.0, KL_2 = 342.3587581253207\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/14000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 15000, Validation = 0.9918000102043152, Training = 1.0, Loss = 5.43411488251877e-07, KL-Loss = 0.0, KL_2 = 342.62411222112877\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/15000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 16000, Validation = 0.9922000169754028, Training = 1.0, Loss = 7.833542440494057e-06, KL-Loss = 0.0, KL_2 = 342.6326975950598\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/16000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 17000, Validation = 0.9923999905586243, Training = 1.0, Loss = 3.9533139783998195e-07, KL-Loss = 0.0, KL_2 = 342.64159116190126\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/17000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 18000, Validation = 0.9926000237464905, Training = 1.0, Loss = 6.868016839689517e-07, KL-Loss = 0.0, KL_2 = 342.6522678448668\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/18000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 19000, Validation = 0.9923999905586243, Training = 1.0, Loss = 1.4249134494548343e-07, KL-Loss = 0.0, KL_2 = 342.66541007111516\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/19000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 20000, Validation = 0.9923999905586243, Training = 1.0, Loss = 2.2491174433980632e-07, KL-Loss = 0.0, KL_2 = 342.6804014548255\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/20000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 21000, Validation = 0.9922000169754028, Training = 1.0, Loss = 1.006241973300348e-06, KL-Loss = 0.0, KL_2 = 342.698518795198\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/21000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 22000, Validation = 0.9924800038337708, Training = 1.0, Loss = 2.686833511233999e-07, KL-Loss = 0.0, KL_2 = 342.71877147216367\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/22000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 23000, Validation = 0.9923999905586243, Training = 1.0, Loss = 8.195588918624708e-08, KL-Loss = 0.0, KL_2 = 342.742480312481\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/23000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 24000, Validation = 0.9926000237464905, Training = 1.0, Loss = 1.5599570701851917e-07, KL-Loss = 0.0, KL_2 = 342.76850059962595\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/24000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 25000, Validation = 0.9925400137901306, Training = 1.0, Loss = 1.8160747572437685e-08, KL-Loss = 0.0, KL_2 = 342.79700844383296\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/25000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 26000, Validation = 0.9923999905586243, Training = 1.0, Loss = 1.676378502679654e-08, KL-Loss = 0.0, KL_2 = 342.82740947306064\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/26000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 27000, Validation = 0.9927200078964233, Training = 1.0, Loss = 1.6763792132223898e-08, KL-Loss = 0.0, KL_2 = 342.8599458388815\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/27000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 28000, Validation = 0.992680013179779, Training = 1.0, Loss = 1.4901141653922423e-08, KL-Loss = 0.0, KL_2 = 342.89441941728165\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/28000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 29000, Validation = 0.9926000237464905, Training = 1.0, Loss = 5.1222723840282924e-09, KL-Loss = 0.0, KL_2 = 342.930610070468\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/29000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 30000, Validation = 0.992520010471344, Training = 1.0, Loss = 3.7252889661942845e-09, KL-Loss = 0.0, KL_2 = 342.96805550907504\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/30000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 31000, Validation = 0.9927999973297119, Training = 1.0, Loss = 2.7939670577126208e-09, KL-Loss = 0.0, KL_2 = 343.00666767798555\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/31000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 32000, Validation = 0.9927999973297119, Training = 1.0, Loss = 6.053594070465351e-09, KL-Loss = 0.0, KL_2 = 343.04715101813747\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/32000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 33000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.08677582089626\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/33000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 34000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.1269729819171\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/34000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 35000, Validation = 0.9927999973297119, Training = 1.0, Loss = 9.313225191043273e-10, KL-Loss = 0.0, KL_2 = 343.16571723350347\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/35000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 36000, Validation = 0.9927999973297119, Training = 1.0, Loss = 4.6566125955216364e-10, KL-Loss = 0.0, KL_2 = 343.1987599162485\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/36000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 37000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.23056982735204\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/37000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 38000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.26453708882804\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/38000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 39000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.30079378335233\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/39000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 40000, Validation = 0.9927999973297119, Training = 1.0, Loss = 4.6566125955216364e-10, KL-Loss = 0.0, KL_2 = 343.3461861962905\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/40000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 41000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.40309181580795\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/41000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 42000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.46994766357375\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/42000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 43000, Validation = 0.9927999973297119, Training = 1.0, Loss = 0.0, KL-Loss = 0.0, KL_2 = 343.55740840877417\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/43000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 44000, Validation = 0.9886000156402588, Training = 1.0, Loss = 0.00012657410115934908, KL-Loss = 0.0, KL_2 = 345.509524316686\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/44000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n",
            "Iteration 45000, Validation = 0.9918000102043152, Training = 1.0, Loss = 2.31487319979351e-06, KL-Loss = 0.0, KL_2 = 345.93011958419123\n",
            "INFO:tensorflow:/scratch/mh740/compression_models/Lenet5/pretrain/45000.ckpt is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txGr1qh_NaK2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}